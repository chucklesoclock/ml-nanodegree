import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
import argparse
from collections import OrderedDict
import inspect


class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)  # Set the agent in the environment
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning  # Whether the agent is expected to learn
        self.Q = dict()  # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon  # Random exploration factor
        self.alpha = alpha  # Learning factor

        # TODO: Set any additional class parameters as needed

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)

        # TODO: Update epsilon using a decay function of your choice
        # TODO: Update additional class parameters as needed
        # TODO: If 'testing' is True, set epsilon and alpha to 0

        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the
            environment. The next waypoint, the intersection inputs, and the deadline
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint()  # The next waypoint
        inputs = self.env.sense(self)  # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ###########
        ###########

        # NOTE: you are not allowed to engineer features outside of the inputs available.
        # Because the aim of this project is to teach Reinforcement Learning, we have placed
        #   constraints in order for you to learn how to adjust epsilon and alpha,
        #   and thus learn about the balance between exploration and exploitation.
        # With the hand-engineered features, this learning process gets entirely negated.

        # TODO: Set 'state' as a tuple of relevant data for the agent
        state = None

        return state

    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        # TODO: Calculate the maximum Q-value of all actions for a given state

        maxQ = None

        return maxQ

    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        # TODO: When learning, check if the 'state' is not in the Q-table
        # TODO: If it is not, create a new dictionary for that state
        #   TODO: Then, for each action available, set the initial Q-value to 0.0

        return

    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        # not learning means totally random action
        if not self.learning:
            action = random.choice(self.valid_actions)
        else:
            action = None

        # TODO: When learning, choose a random action with 'epsilon' probability
        # TODO: Otherwise, choose an action with the highest Q-value for the current state
        # TODO: Be sure that when choosing an action with highest Q-value that you randomly select between actions that "tie".
        return action

    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives a reward. This function does not consider future rewards
            when conducting learning. """

        # TODO: When learning, implement the value iteration update rule
        #   TODO: Use only the learning rate 'alpha' (do not use the discount factor 'gamma')

        return

    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        state = self.build_state()  # Get current state
        self.createQ(state)  # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action)  # Receive a reward
        self.learn(state, action, reward)  # Q-learn

        return


def run():
    """ Driving function for running the simulation.
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    all_flags = command_line_parse()

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment(**all_flags['env'])

    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent, **all_flags['agent'])

    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent, **all_flags['follow'])

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, **all_flags['sim'])

    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(**all_flags['run'])


def categorize_flags(set_flags):
    """
        gives categorized keyword argument dict that functions inside run() can unpack for user-set flags
        :param set_flags: argparse parser output of user-set flags
        :type set_flags: dict
        :rtype dict
        :returns dict of section flags
        """
    user_flags = {cat: {flag: set_flags[flag] for flag in flag_config[cat]} for cat in flag_config}
    return user_flags


smartcab_sim_run_funcs = (Environment.__init__,
                          LearningAgent.__init__,
                          Environment.set_primary_agent,
                          Simulator.__init__,
                          Simulator.run)


def set_all_defaults(parser, *funcs):
    """
    Set default values from function declaration so there's no mix up.

    Warning: Issues may occur if default function values goes from True -> False or vice versa as in argument actions
    in `flags` have to change from 'store_false' to 'store_true' and vice versa :type parser: argparse.ArgumentParser
    """
    assert all(callable(f) for f in funcs)
    assert isinstance(parser, argparse.ArgumentParser)
    defaults = {}
    for f in funcs:
        defaults.update(get_defaults(f))
    parser.set_defaults(**defaults)


def get_defaults(func):
    getargspec = inspect.getargspec
    info = getargspec(func)
    d = len(info.defaults)
    return {k: v for k, v in zip(info.args[-d:], info.defaults)}


flag_categories = ['env', 'agent', 'deadline', 'sim', 'run']
flag_config = dict(
    env={
        'verbose': {
            'names': ('-v', '--verbose'),
            'kwargs': dict(action='store_true', help='generates additional output from the simulation')
        },
        'num_dummies': {
            'names': ('-N', '--num_dummies'),
            'kwargs': dict(type=int, metavar=('INT'), help='number of dummy agents in the environment')
        },
        'grid_size': {
            'names': ('-g', '--grid_size'),
            'kwargs': dict(nargs=2, type=int, metavar=('COLS', 'ROWS'),
                           help='controls the number of intersections = columns * rows')
        }
    },
    agent={
        'learning': {
            'names': ('-l', '--learning'),
            'kwargs': dict(action='store_true', help='forces the driving agent to use Q-learning')
        },
        'epsilon': {
            'names': ('-e', '--epsilon'),
            'kwargs': dict(type=float, metavar=('FLOAT'), help='NO EFFECT without -l: value for the exploration factor')
        },
        'alpha': {
            'names': ('-a', '--alpha'),
            'kwargs': dict(type=float, metavar=('FLOAT'), help='NO EFFECT without -l: value for the learning rate')
        }
    },
    deadline={
        'enforce_deadline': {
            'names': ['-D', '--deadline'],
            'kwargs': dict(action='store_true', dest='enforce_deadline',
                           help='enforce a deadline metric on the driving agent')
        }
    },
    sim={
        'update_delay': {
            'names': ('-u', '--update-delay'),
            'kwargs': dict(type=float, metavar=('SECONDS'), help='time between actions of smartcab/environment')
        },
        'display': {'names': ('-d', '--display'), 'kwargs': dict(action='store_false', help='disable simulation GUI')},
        'log_metrics': {
            'names': ('-L', '--log'),
            'kwargs': dict(action='store_true', dest='log_metrics', help='log trial and simulation results to /logs')
        },
        'optimized': {
            'names': ('-o', '--optimized'),
            'kwargs': dict(action='store_true', help='change the default log file name if optimized')
        }
    },
    run={
        'tolerance': {
            'names': ('-t', '--tolerance'),
            'kwargs': dict(type=float, metavar=('FLOAT'),
                           help='epsilon tolerance before beginning testing after exploration')
        },
        'n_test': {
            'names': ('-n', '--n_test'),
            'kwargs': dict(type=int, metavar=('INT'), help='number of testing trials to perform')
        }
    })
flag_config = OrderedDict(sorted(flag_config.items(), key=lambda t: flag_categories.index(t[0])))


def command_line_parse():
    """gets user-set flags from command line"""
    parser = argparse.ArgumentParser(description='runs the smartcab simulation with various options',
                                     usage='smartcab/agent.py [-h] [-v]'
                                           '\n   env flags:     [-N <dummies> -g <cols> <rows>]'
                                           '\n   drive flags:   [-l [-a <float> -e <float>]'
                                           '\n   deadline flag: [-D]'
                                           '\n   sim flags:     [-dLo -u <delay_secs>]'
                                           '\n   run flags:     [-t <tolerance> -n <tests>]',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    helps = ['environment/world options', 'driving agent options', 'deadline option',
             'simulation options', 'run-time experiment options']
    arg_groups = {key: parser.add_argument_group(text) for key, text in zip(flag_categories, helps)}
    for group in flag_config:
        for arg_name in flag_config[group]:
            d = flag_config[group][arg_name]
            arg_groups[group].add_argument(*d['names'], **d['kwargs'])
    set_all_defaults(parser, *smartcab_sim_run_funcs)
    flat_flags = vars(parser.parse_args())
    flat_flags['grid_size'] = tuple(flat_flags['grid_size'])
    return categorize_flags(flat_flags)


if __name__ == '__main__':
    run()
